{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "We start by importing the necessary libraries for our analysis. Here's what each library is used for:\n",
    "\n",
    "- `pandas`: Used for data manipulation and analysis.\n",
    "- `matplotlib.pyplot`: Provides a MATLAB-like plotting framework.\n",
    "- `seaborn`: A statistical data visualization library based on matplotlib, used for creating attractive and informative statistical graphics.\n",
    "- `MinMaxScaler`, `Normalizer`, `StandardScaler` from `sklearn.preprocessing`: These are tools for data preprocessing, specifically for scaling numerical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "In this section, we load the dataset from a CSV file named `tv_shows.csv`.\n",
    "We use the `pd.read_csv()` function from the pandas library to read the CSV file and store the data in a DataFrame named `tv_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_data = pd.read_csv('tv_shows.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "### Viewing the First Few Rows\n",
    "\n",
    "We start by displaying the first few rows of the dataset using the `head()` method. This gives us a quick glimpse of what the data looks like and helps us understand its structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data Columns\n",
    "\n",
    "Here, we retrieve the column names of the dataset using the columns attribute. This provides a list of all the columns present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = tv_data.columns\n",
    "print(\"Data columns\",data_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data Information\n",
    "\n",
    "We use the info method to get a concise summary of the dataset, including the number of non-null values and data types of each column. This helps us understand the completeness and structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = tv_data.info\n",
    "print(\"The data info\",data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data Shape\n",
    "\n",
    "The shape attribute gives us the dimensions of the dataset, i.e., the number of rows and columns. This helps us understand the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shape = tv_data.shape\n",
    "print(\"The data shape\",data_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics\n",
    "\n",
    "We use the describe() method to generate descriptive statistics of the numerical columns in the dataset, such as count, mean, standard deviation, minimum, and maximum values. This provides insights into the central tendency, dispersion, and shape of the distribution of numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types\n",
    "\n",
    "The dtypes attribute gives us the data types of each column in the dataset. Understanding the data types is crucial for data preprocessing and analysis, as it determines the operations we can perform on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "In this section, we perform data cleaning tasks to ensure the quality and integrity of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all We drop the unnecessary columns that we won't need. In this case the Id and the unamed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_data.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "tv_data.drop(['ID'], axis=1, inplace=True)\n",
    "\n",
    "print(tv_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we check for duplicate rows in the dataset. Duplicate rows can skew analysis results and should be removed if found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate = tv_data.duplicated().sum()\n",
    "print(\"Number of Duplicates: \", duplicate)\n",
    "if duplicate > 0:\n",
    "    print(tv_data[tv_data.duplicated()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also examine the number of unique values in each column to understand the diversity and uniqueness of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_data['Year'].unique()\n",
    "tv_data['Age'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for missing Values\n",
    "\n",
    "We start by checking for any missing values in the dataset. Missing values can hinder analysis and modeling, so it's essential to identify and handle them appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = tv_data.isnull().sum()\n",
    "print(\"The missing values in the dataset are\",missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the missing values from the column age \n",
    "# tv_data.dropna(subset=['age'], inplace=True)\n",
    "# print(\"Data after dropping NA values for age\",tv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These function is converting the String value of the IMDb rating  into a Float by  removing any non-numeric characters in this case \"/\" and then taking the first part before / and devide it by the second part and multiply the result by ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rating(rating):\n",
    "    if isinstance(rating, str):  \n",
    "        parts = rating.split('/')\n",
    "        if len(parts) == 2:\n",
    "            return float(parts[0]) / float(parts[1]) * 10\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply our convertion  function to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_data['IMDb'] = tv_data['IMDb'].apply(convert_rating)\n",
    "\n",
    "print(tv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle missing values, we'll replace them with the mean of the IMDb column and place where there are missing values. This helps us maintain the integrity of the dataset while ensuring that missing values are appropriately accounted for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean IMDb rating excluding null values\n",
    "mean_rating = tv_data['IMDb'].mean(skipna=True)\n",
    "\n",
    "# Replace null values with the mean IMDb rating\n",
    "tv_data['IMDb'].fillna(mean_rating, inplace=True)\n",
    "\n",
    "print(tv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we verify now that IMDb column has no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values_after = tv_data.isnull().sum()\n",
    "print(\"Null values after replacing with mean:\\n\", null_values_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "\n",
    "In this section, we perform data transformation tasks to prepare the dataset for analysis.\n",
    "\n",
    "\n",
    "We start by cleaning the 'Age' column, which likely contains characters such as '+' and 'all', as well as missing values (NaN). We replace '+' and 'all' with '' (empty string) and replace NaN with 0 to ensure consistency and completeness.\n",
    "\n",
    "and then we convert the type of the data from String to integers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '+' and 'all' with '' in the 'Age' column and replace NaN with 0\n",
    "tv_data['Age_num'] = tv_data['Age'].str.replace('+', '').replace('all', '1').fillna(0)\n",
    "\n",
    "# Convert the 'Age_num' column to integers\n",
    "tv_data['Age_num'] = tv_data['Age_num'].astype(int)\n",
    "\n",
    "\n",
    "##print the values of the column Age_num\n",
    "print(tv_data['Age_num'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection\n",
    "\n",
    "In this section, we check for outliers in the dataset.\n",
    "\n",
    "Outliers are data points that significantly differ from other observations in the dataset. They can arise due to various reasons, such as measurement errors, data entry mistakes, or genuine anomalies in the data. Outliers can skew statistical analyses and machine learning models, leading to misleading results if not properly handled.\n",
    "\n",
    "### Checking for Outliers\n",
    "\n",
    "We use visualizations and statistical techniques to identify outliers in the dataset. One common method is to use box plots and histogranms, which display the distribution of numerical variables and highlight any data points that fall outside the whiskers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A histogram provides a visual representation of the distribution of values in the 'Age_num' column. It can help you see the frequency of different age groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tv_data['Age_num'], bins=20, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization\n",
    "\n",
    "\n",
    "#### **1-Create Scalers:**\n",
    "\n",
    "Think of a scalar as a tool that helps us adjust the values of our data to make them easier to work with. We're creating two types of scalers here: one for Min-Max scaling and another for feature normalization.\n",
    "\n",
    "\n",
    "#### **2-Scale Data:**\n",
    "Now, we want to change the values in our data to make them fit a specific range or pattern. For example, if we have ratings that go from 1 to 10, we might want to change them so they go from 0 to 1 instead. That's what scaling does.\n",
    "With Min-Max scaling, we're adjusting our values to fit between 0 and 1.\n",
    "With feature normalization, we're adjusting our values so that each row of data has a \"length\" of 1, which makes it easier to compare different rows.\n",
    "\n",
    "#### **3-Add Scaled Features Back to DataFrame:**\n",
    "We've now transformed our data, but we want to keep track of both the original and scaled values. So, we're adding two new columns to our data frame: 'IMDb_MinMax' and 'IMDb_Normalize'.\n",
    "These new columns will hold the scaled values of our original 'IMDb' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create scalers\n",
    "scaler_minmax = MinMaxScaler()\n",
    "scaler_normalize = Normalizer()\n",
    "\n",
    "#scale data\n",
    "data_minmax = scaler_minmax.fit_transform(tv_data[['IMDb']])  #rescale to [0,1] range\n",
    "\n",
    "# explain to me what the normalize does and how it works?\n",
    "data_normalize = scaler_normalize.fit_transform(tv_data[['IMDb']]) \n",
    "\n",
    "#add scaled features back to the data frame\n",
    "tv_data['IMDb_MinMax'] = data_minmax\n",
    "tv_data['IMDb_Normalize'] = data_normalize\n",
    "\n",
    "print(\"Data with min-max scaling\")\n",
    "print(tv_data.head())\n",
    "\n",
    "print(\"\\n\\nData with feature normalization\")\n",
    "print(tv_data.tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving our cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_data.to_csv('tv_shows_scaled.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
